# Summary
All Model Links Can Be found Here : [PETRAINED MODELS](https://huggingface.co/transformers/pretrained_models.html) 

## Model:  **RoBERTa-base** [A Robustly Optimized BERT Pretraining Approach]

  -Accuracy
  
    85.07109004739336%

  -Notebook
  
    Mod_Roberta_Acc_85_Param_120M.ipynb

  -Details
  
    12-layer, 768-hidden, 12-heads, 125M parameters RoBERTa using the BERT-base architecture

## Model: DistilRoBERTa [Remove Insignificant Parameters from RoBERTa]

  -Accuracy
  
    82.91205897840969 %
  
  -Notebook
  
    https://github.com/BinarySwami-10/JNU-ML-Final-Project/blob/main/Mod_distilroberta_Acc_82_Param_80M.ipynb
    
  -Details
       	
    6-layer, 768-hidden, 12-heads, 82M parameters, The DistilRoBERTa model distilled from the RoBERTa model roberta-base checkpoint.


## Model: XLNet-base [Generalized Autoregressive Pretraining for Language Understanding]

  -Accuracy
  
    79%

  -Notebook
  
    https://github.com/BinarySwami-10/JNU-ML-Final-Project/blob/main/Mod_XLnet_Acc_79_Param_110M.ipynb
    
  -Details
    
    12-layer, 768-hidden, 12-heads, 110M parameters. XLNet English model

## Model: XLNet-Large [Generalized Autoregressive Pretraining for Language Understanding]

  -Accuracy
    
    75.92550265792721 %
  
  -Notebook
    
    https://github.com/BinarySwami-10/JNU-ML-Final-Project/blob/main/Mod_XLnet_large_Acc_75.ipynb
  
  -Details
    
    24-layer, 1024-hidden, 16-heads, 340M parameters. XLNet Large English model

