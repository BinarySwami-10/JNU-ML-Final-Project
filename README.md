# Summary
All Model Links Can Be found Here : [PETRAINED MODELS](https://huggingface.co/transformers/pretrained_models.html) 

## Model:  **RoBERTa** [A Robustly Optimized BERT Pretraining Approach]

  Accuracy:
  
    85%

  Notebook Name : 
  
    Mod_Roberta_Acc_85_Param_120M.ipynb

  Details : 
  
    12-layer, 768-hidden, 12-heads, 125M parameters RoBERTa using the BERT-base architecture
