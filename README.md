# Summary

## Model:  RoBERTa [A Robustly Optimized BERT Pretraining Approach]

  Notebook Name : 
    Mod_Roberta_Acc_85_Param_120M.ipynb

  Details : 
    12-layer, 768-hidden, 12-heads, 125M parameters RoBERTa using the BERT-base architecture
